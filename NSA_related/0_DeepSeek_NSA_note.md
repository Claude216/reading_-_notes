### First Iteration:

- Base: 
  
  - inherent sparsity of softmax attention
    
    - selective computation on critical query-key pairs 

- Two key challenges:
  
  - **Hardware-aligned inference speedup**
    
    - prefilling
    
    - decoding
  
  - **Training-aware algorithm design**
    
    - end-to-end computation with trainable operators (?)

- NSA structure: 
  
  - input sequences are split to *continuous blocks*
    
    - **compressed attention** for *coarse-grained patterns* (?)
      - (*answer*) **coarse-grained patterns** are like "summaries" of token blocks which are generated by the model through a **learned compression process**. The strucutre splits sequence into different blocks and by utilizing the coarse-grained patterns, we can reduce the computation from token-wise to block-wise for efficient GPU memory access. 
      - (*follow-up*) how this **learned compression process** generate the pattern?
        - using a trainable MLP to produce a single vector which can *represent* the block's *high-level semantics* with adaptivity to prioritize salient patterns. 
    - **selected attention** for important *token blocks*
    - **sliding attention** (?) for *local context*
      - (*answer*) A localized attention mechanism: each query token attends **only** to a **fixed window of recent tokens** to capture immediate, short-range contextual patterns. 
        - Like a selective computation to increase the efficiency: O($t^2$) to $O(w * t)$ 
  
  - innovations: 
    
    - *Hardware-aligned System*: 
      
      - Optimize *blockwise sparse attention for Tensor Core utilization* and *memory access*
        
        - Balance atrithmetic intensity
      
      - Why it can balance arithmetic intensity (?)
        
        - (*answer*) From two aspects: 
          
          1. BSATCU (Computation)
             
             - Problem: *Sparse* attention often leads to irregular memory access patterns (e.g., scattered non-zero attention scores), which underutilize Tensor Cores optimized for dense, contiguous matrix operations
             
             - Solution: 
               
               - Process attention in **contiguous blocks** (e.g., l=32 tokens per block) and groups queries by their shared sparse key/value blocks ----> **coalesced memory access**, which allows Tensor Cores to efficiently execute matrix multiplications on aligned, dense blocks.
               
               - Ex: loading a block **once** and **reuse** it for all queries requiring that block.
          
          2. MAO (Memory)
             
             - Problem: Sparse attention methods often reduce compute but not memory access (e.g., many loadings for small, non-contiguous KV cache fragments)
             
             - Solution: 
               
               - **Group-Centric Data Loading**: queries within a GQA group share the same **sparse KV blocks** across multiple queries. 
               
               - **Hierarchical Sparsity**: Combining *coarse-grained (compressed)* and *fine-grained (selected)* tokens minimizes total KV cache size while retraining critical information. 
        
        - (cont) Arithmetic Intensity = $\frac {Compute \space Operations \space (FLOPS)}  {Memory \space Access \space Bytes}$ 
          
          - Maximizes Tensor Core thoughput via *bloackwise dense operations*
          
          - Reduces HBM accesses via *block resue and hierarchical sparsity* 
    
    - *Training-aware design*: 
      
      - Enable stable end-to-end training through efficient algorithms and backward operators
      - What is "end-to-end training" (?)
        - ï¼ˆ*answer*) Combines all steps into a single trainable model. The entire network (from raw input to final output) is jointly optimized, which can lead to improved performance as the system learns the best internal representations directly from data. 
      - Why end-to-end training is important (?)
        - (*answer*): 
          - Traditional Pipeline: Divides the process into hand-craft processing where each step is designed separately and may include non-trainable modules or heuristics.
          - By utilizing the end-to-end training:
            - **Unified Optimization**: Every layer is trainable so model can learn optimal filters/feature representations/decision boundaries in *one unified process*
            - **No Hand-crafted Steps**: There are no separate fixed modules or intermediate steps that rely on pre-defined heuristics
            - **Adaptability**: The network can adapt to complex data distributions and variations. 
      - Why end-to-end training can be achieved through the above description (?)
    
    - Result of innovations: 
      
      - Enable NSA to support both efficient deployment and end-to-end training 
        
        
        
        

### Second Iteration:

#### Rethinking Sparse Attention Methods:

- Modern sparse attention methods have made some achievements
  
  - Apply sparsity during inference while retraining a **pretrained Full Attention backbone**, potentially introducing architectural bias that limits their ability to fully exploit sparse attention's advantages.  
1. Two challenges on inference latency reductions
   
   - **Phase-Restricted Sparsity** 
     
     - methods like *H2O* apply sparsity during autoregressive decoding while requiring computationally intensive pre-processing (attention map calculation, index building) during prefilling. 
       
       - (*question*) H2O?
       
       - (*question*) attention map calculation?
       
       - (*question*) index building?
       
       - (*question*) prefilling? 
     
     - approaches like Minference focus solely on prefilling sparsity. 
       
       - (*question*) why focusing solely on prefilling sparsity is not good enough?
     
     - Above methods fail to achieve acceleration across **all inference stages** 
       
       - (*question*) how many phases are there? 
   
   - **Incompatibility with Advanced Attention Architecture**
     
     - (*question*) MQA (Multiple-Query Attention)?
     
     - (*question*) Grouped-Query Attention (GQA)? 
       
       - Above two architectures reduce the memory acess bottleneck during decoding by **sharing KV across multiple query heads**
       
       - (*question*) how this implemented? 
     
     - Disadvantage of some sparse attention methods: *their scattered memory access pattern conflicts with the efficient memory access design from advanced architectures*
     
     - Current sparse attention methods focus on KV-cache reduction or theoretical computation reduction, but struggle to achieve significant latency reduction in advanced framework or backends. 
     
     - Direction: Algorithmes that combine both **advanced architectural** and **hardware-efficient implementation** to fully leverage sparsity for improving model efficiency. 

2. **Myth of Trainable Sparsity**
   
   - Two key insights from analyzing inference-only approaches:
     
     - **Performance Degradation**: *Applying sparsity post-hoc forces models to deviate from their pretrained optimization trajectory*. 
       
       - (*question*) Applying sparsity post-hoc?
       
       - (*question*) how this forces models to deivate? 
       
       - (*question*) what does the "vulnerable" mean here when it comes to structures to pruning during inference? 
     
     - **Training Efficiency Demands**: *Effificent handling of long-sequence training is crucial for modern LLM development*. 
       
       1. Pretraining on longer doc to enhance model capacity
       
       2. Subsequent adaptation phases like long-context fine-tuning and reinforcement learning.
       - Existing sparse attention methods primarily target inference, leaving the **computational challenges in training largely unaddressed.** 
     
     - Challenges on adapting existing sparse attention for training: 
       
       1. **Non-Trainable Components**: 
          
          - Discontinuities in computational graph
            
            - (*question*) ClusterKV
            
            - (*question*) MagicPIG
          
          - Prevent gradient flow through the token selection process
          
          - Limit the model's ability to learn optimal sparse patterns
       
       2. **Inefficient Back-propagation**: 
          
          - Token-granular selection strategy used in approaches like HashAttention --> the need to load a large number of individual tokens from the KV cache during attention computation. 
            
            - (*Question*) HashAttention?
          
          - "Non-contiguous memory access" of the strategy blocks the adaption of faster attention tech which rely on "contiguous memory access" and "blockwise computation"
            
            - (*Question*) FastAttention? 
          
          - The crucial need of high throughput. 

3. **Native Sparsity as an Imperative** 
   
   - For the redesign of sparse attention mechanisms ----> NSA
     
     

#### Methodology

*Algorithem design & kernel optimization*

1. Background: The disadvantage of the current Full Attention mechanism
   
   - Arithmetic Intensity: ratio of compute operations to memory accesses.
     
     - Determined by its *peak compute capability* and *memory bandwidth* 
   
   - Training and prefilling phases constrans by the computation cost ----> **compute-bound**
   
   - Decoding constrains by the memory access ----> **memory-bound**
     
     - (*question*) why training and prefilling limited by computation cost?
       
       - (*answer* ?) batched amtrix multiplications and attention computations
     
     - (*question*) why decoding phase limited by memory access bound? 
       
       - (*answer* ?) generate one token per forward pass while requiring loading the *entire* key-value cache

2. Overall Framework: 
- Replacing the traditional kv pair with new set of represnetation kv pairs
  
  - ![2ec23930-c599-4a99-9375-cc4c944609c1](file:///C:/Users/llxCl/OneDrive/%E5%9B%BE%E7%89%87/Typedown/2ec23930-c599-4a99-9375-cc4c944609c1.png)
  
  - ![e3e2e7df-f55e-4f83-9c4e-65564f719b15](file:///C:/Users/llxCl/OneDrive/%E5%9B%BE%E7%89%87/Typedown/e3e2e7df-f55e-4f83-9c4e-65564f719b15.png)

- Along with the mapping stragegies: *C = {cmp, slc, win}* for compression, selection, and sliding window for keys and values.
  
  - $g^c_t \in [0,1]$ is the gate score for corresponding strategy c, derived from input features via an MLP and sigmoid activation 
  
  - $N_t = \sum_{c \in C} size[\~K_t^c]$ 
  
  - ensuring $N_t \ll t$ to maintain a high sparsity ratio
3. Algorithm Design
- **Token Compression**
  
  - Idea: Obtain *compressed keys and values* that capture the info of the entire block by aggregating sequential blocks of keys or values into block-level representations.
    
    - $\~K_t^{cmp} = f_K^{cmp}(\bold{k}_{:t}) = \{\varphi(\bold{k}_{id+1:id+l}) | 1 \le i \le \lfloor \frac{t - l} d \rfloor \} $ 
      - $l$: block length
      - $d$: sliding stride between adjacent blocks
      - $\varphi$: a learnable MLP with intra-block position encoding to map keys in a block to a single compressed key. 
      - $\~K_t^{cmp} \in \bold{\R}^{d_k \times \lfloor \frac {t-l} d \rfloor}$: tensor composed by compresion keys. 
    - Adopt $d < l$ to **mitigate info fragmentation** 
      - (*question*) why we want to mitigate information fragmentation? why making $d < l$ can achieve this?
    - *Compressed representations capture **coarser-grained high-level semantic information** and reduce computational burden of attention* 
      - (*idea*) i am actually thinking about tree, like in a full attention, each token is like the leaf point, while now, NSA does things one or two level above the leaf point. 

- **Token Selection**
  
  - Idea: using only the compressed keys, we may **lose** *important fine-grained information* $\rarr$ **selectively preserve individual keys and values**. 
    
    - **Identification & Preservation** on the most relevant tokens with *low computational overhead*
  
  - ***Blockwise Selection***
    
    - selection processes key and value sequences in spacial continuous blocks $\larr$ Two factors: hardware efficiency consideration; inherent distribution patterns of attention scores.
    
    - Why blockwise selection is so crucial?
      
      - *modern GPU architectures exhibit significantly higher throughput for continuous block accesses* compared to random index-based reads. Also, *blockwise computation enables optimal utilization of Tensor Cores*.
        
        - (*question*)  *FlashAttention*?
      
      - *blockwise selection follows the inherent distribution patterns of attention scores* 
        
        - "neighboring keys tend to share similar importance levels"
      
      - Implementation: 1. divide key, value sequences into selection blocks. 2. Identify the most important blocks for attention computation by assigning importance scores to each block. 
  
  - ***Importance Score Computation***
    
    - leverage the **intermediate attention scores** produced from the attention computation of compression tokens to induce selection block importance scores
      
      - $\bold{p}_t^{cmp} = Softmax(\bold{q}_t^T \~K_t^{cmp})$ 
        
        - $\bold{p}_t^{cmp} \in \R^{\lfloor \frac {t-l} d \rfloor}$: attention scores between $q_t$ and compression keys $\~K_t^{cmp}$. 
    
    - Let $l'$ denote the selection block size and when compression blocks and selection blocks share the **same** blocking scheme, $\bold{p}_t^{slc} = \bold{p}_t^{cmp}$ 
    
    - When the blocking schemes differ, derive the importance scores for selection blocks according to their *spatial relationship*
      
      - (*question*) how does the spatial relationship be calculated
      
      - $p_{t}^{\mathrm{slc}}[j] = \sum_{m=0}^{\frac{l'}{d}-1} \sum_{n=0}^{\frac{l'}{d}-1} p_{t}^{\mathrm{cmp}}\!\Bigl[\tfrac{l'}{d}\,j + m + n\Bigr]$  
    
    - GQA and MQA would share key-value caches across query heads, and consistent block selection across these heads has to be ensured to minimize KV cache loading during decoding. 
    
    - Shared importance scores across heads in a group: $\bold{p}_t^{slc'} = \sum_{h=1}^{H} p_t^{slc,(h)}$. 
      
      - (h): head index
      
      - H: # of query heads in each group
      
      - ensures *consistent block selection* across heads within the same group
  
  - ***Top-n Block Selection***: 
    
    - $I_t = \{i | rank(\bold{p}_t^{slc'} [i]) \le n \}$ 
    
    - $\~K_t^{slc} = Cat[\{\bold{k}_{il' + 1: (i+1)l'} | i \in I_t \}]$ 
    
    - rank = 1 corresponding to the highest score
    
    - $I_t$: the set of selected blocks' indices
    
    - Cat: concatenation operation
    
    - $\~K_t^{slc} \in \R^{d_k \times nl'}$: tensor composed by compresion keys.

- **Sliding Window** 
  
  - local patterns typically adapt faster and can dominate the learning process, potentially preventing the model from effectively learning from compression and selection tokens. 
  
  - Intro of a dedicated sliding window branch  that explicitly handles local context, allowing other branches (compression and selection) to focus on learning their respective features without *being shortcutted by local patterns*. 
  
  - Maintain: 
    
    - ![0f131a36-7564-43f3-8e05-71fc77c1d121](file:///C:/Users/llxCl/OneDrive/%E5%9B%BE%E7%89%87/Typedown/0f131a36-7564-43f3-8e05-71fc77c1d121.png)
    
    - in a window $w$
  
  - Isolate attention computation of different info sources (compression tokens, selected tokens, sliding window)
  
  - Branch outputs then aggregate **through** a **learned gating mechanism**. 
  
  - **Independent keys and values** are provided for the three branches to prevent **shortcut learning across attention branches with marginal computational overhead**. 
  
  - This also enable **stable learning** by **preventing gradient interference** between local and long-range pattern recognition, while intro minimal overhead.
4. Kernel Design: 
   
   - A hardware-aligned sparse attention kernels upon Triton. 
   
   - MHA is memory-intensive and inefficient for decoding thus focus on architectures with *shared KV caches* like GQA and MQA.
   
   - FlashAttention-2 kernels: loading temporally continuous query blocks into SRAM $\rarr$ inefficient memory access since *queries within a block may require disjoint KV blocks* 
     
     - (*question*) why would the requirement for disjoint KV blocks happen on queries in the strategy of FlashAttention? 
   
   - A different query grouping strategy: 
     
     - "for each position on the query sequence, we load all query heads within a GQA group (which share the same sparse KV blocks) into SRAM."
   
   - Key features of hte kernel architecture: 
     
     - **Group-Centric Data Loading**: for each inner loop, load all heads' queries $Q \in \R^[h,d_k]$ in the group at position $t$ and their shared sparse key / value block indices $I_t$
     
     - **Shared KV Fetching**: In the inner loop, sequentially load continuous key / value blocks indexed by $I_t$ into SRAM as $K \in \R^{[b_k, d_k]}, V \in \R^{[b_k, d_v]}$ to minimize memory loading, where $B_k$ is the kernel block size satisfying $B_k|l'$. 
     
     - **Outer Loop on Grid**: Since the inner-loop length (proportional to the selected block count n) remains nearly identical for different query blocks, we put *query / output loops in Triton's grid shceduler to simplify and optimize the kernel*. 
     
     - Achieve near-optimal arithmetic intensity by: 
       
       - **eliminating redundant KV transfers through group-wise sharing**
       
       - **balancing compute workloads across GPU streaming multiprocessors**
     
     - (*question*) SRAM? HBM? Triton?

#### Experiments:

- benchmarks: 
  
  - knowledge (MMLU, MMLU-PRO, CMMLU)
  
  - reasoning (BBH, GSM8K, MATH, DROP)
  
  - coding (MBPP, HumanEval)

- baseline methods: 
  
  - H2O
  
  - infLLM
  
  - Quest
  
  - Exact-Top
  
  - Full Attn

- Performance Comparison: 
  
  - Long-Context Eval
  
  - Chain-of-Thought Reasoning Eval

- Efficiency Analysis: 
  
  - hardware: 8-GPU A 100 system
  
  - training speed
  
  - decoding speed

#### Related Works:

- Fixed Sparse Pattern: StreamingLLM

- Dynamic Token Pruning: H2O, SnapKV

- Query-Aware Selection: Quest, InfLLM, ClusterKV
