- {{H2O: Heavy-Hitter Oracle for Efficient GenerativeInference of Large Language Models}}

- {{Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention.}}

- {{Chainof-thought prompting elicits reasoning in large language models.}}

- {{Fast transformer decoding: One write-head is all you need.}}

- {{GQA: Training Generalized Multi-Query Transformer Models fromMulti-Head Checkpoints.}}

- {{ClusterKV: Manipulating LLM KV Cache inSemantic Space for Recallable Compression.}}

- {{MagicPIG: LSH Sampling for Efficient LLM Generation.}}

- {{HashAttention: Semantic Sparsity for Faster Inference.}}

- {{Efficient Streaming Language Models with Attention Sinks.}}

- {{SnapKV: LLM Knows What You are Looking for Before Generation.}}

- {{Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference.}}

- {{Mamba: Linear-time sequence modeling with selective state spaces}}

- {{Outrageously large neural networks: The sparsely-gated mixture-of-experts layer}}

- {{Online normalizer calculation for softmax}}

- {{Blockwise Parallel Transformer for Large Context Models}}

- {{Training compute-optimal large language models}}

- {{Reformer: The efficient transformer}}

- {{Memorizing transformers}}

- {{Unlimiformer: Long-range transformers with unlimited length input}}

- {{Colt5: Faster long-range transformers with conditional computation}}

- {{Sparse sinkhorn attention}}

- {{Moa: Mixture of sparse attention for automatic large language model compression}}

- {{SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs}}

- {{Transformers are multi-state rnns}}

- {{Model tells you what to discard: Adaptive kv cache compression for llms}}

- {{LongHeads: Multi-Head Attention is Secretly a Long Context Processor}}

- {{Retentive network: A successor to transformer for large language models}}
